{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMFJLWPW//B8VQE+xsuYW4J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juhi3101/ml_libraries/blob/main/regression_assingmen1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGI4YLk2-f4C"
      },
      "outputs": [],
      "source": [
        "Explain the difference between simple linear regression and multiple linear regression. Provide an\n",
        "example of each.\n",
        "Simple Linear Regression:\n",
        "Simple Linear Regression is used when there is a linear relationship between a single independent variable (X) and a dependent variable (Y).\n",
        "It aims to find the best-fitting linear equation (usually a straight line) that represents the relationship between X and Y.\n",
        "Example of Simple Linear Regression:\n",
        "Let's say you want to predict a person's salary (Y) based on their years of experience (X). You collect data from several individuals,\n",
        "where X is the years of experience, and Y is the corresponding salary. Using simple linear regression, you can find the best-fitting line\n",
        "that describes the relationship between years of experience and salary.\n",
        "\n",
        "Multiple Linear Regression:\n",
        "Multiple Linear Regression is used when there are two or more independent variables (X1, X2, X3, etc.) that collectively influence a dependent variable (Y).\n",
        "It extends the concept of linear regression to model more complex relationships between multiple predictors and the target variable.\n",
        "Example of Multiple Linear Regression:\n",
        "Suppose you want to predict a car's fuel efficiency (miles per gallon, MPG) based on several factors such as engine size (X1),\n",
        "vehicle weight (X2), and horsepower (X3). You collect data from 100 different cars, recording these three independent variables and their\n",
        "corresponding MPG values. You can use multiple linear regression to create a model that estimates MPG based on these three factors"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in\n",
        "a given dataset?\n",
        "\n",
        "Linear regression relies on several key assumptions,here are the main assumptions of linear regression:I\n",
        "Linearity:\n",
        "Assumption: The relationship between the independent variables and the dependent variable is linear.\n",
        "\n",
        "Independence:\n",
        "Assumption: The residuals (errors) should be independent of each other. This means that the error for one observation should not be related to the error\n",
        "for another observation.\n",
        "\n",
        "No Multicollinearity:\n",
        "Assumption: Independent variables in multiple linear regression should not be highly correlated with each other (multicollinearity).\n",
        "High multicollinearity can make it challenging to interpret the individual coefficients.\n",
        "\n",
        "No Endogeneity:\n",
        "Assumption: The independent variables should not be correlated with the error term. This means that there should be no omitted variables that are influencing\n",
        "both the independent variables and the dependent variable."
      ],
      "metadata": {
        "id": "zGJzYv1u_2ca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using\n",
        "a real-world scenario.\n",
        "Interpreting the slope and intercept in a linear regression model is essential for understanding the relationship between the\n",
        "independent and dependent variables:\n",
        "\n",
        "Scenario: Predicting House Prices\n",
        "\n",
        "Suppose we are building a linear regression model to predict house prices (Y) based on the size of the house in square feet (X)\n",
        "\n",
        "Slope (Coefficient of Size): The slope is 100. This means that for each additional square foot increase in the size of a house,\n",
        " the predicted price of the house increases by $100, assuming all other factors remain constant. So, larger houses tend to have higher prices,\n",
        " and the relationship is positive.\n",
        "\n",
        "Intercept: The intercept is $50,000. In this context, it represents the predicted price of a house when its size is zero square feet.\n",
        "However, this interpretation doesn't make sense because houses cannot have a size of zero square feet. Therefore, the intercept may not have a practical\n",
        "interpretation in this scenario."
      ],
      "metadata": {
        "id": "RLaZIa_1A-vP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q4. Explain the concept of gradient descent. How is it used in machine learning?\n",
        "Gradient descent is a fundamental optimization algorithm used in machine learning and deep learning to minimize the loss function of a model\n",
        "by adjusting its parameters iteratively. It's a critical component of training various types of machine learning models, especially those with a\n",
        "large number of parameters, such as neural networks.\n",
        "\n",
        "Gradient descent is used in machine learning in several ways:\n",
        "\n",
        "Model Training: It is the primary method for training machine learning models, including linear regression, logistic regression, support vector machines,\n",
        "and neural networks. During training, the algorithm adjusts the model's parameters to minimize the loss function, making the model more accurate in making\n",
        "predictions.\n",
        "Hyperparameter Tuning: Learning rate is a hyperparameter in gradient descent that can significantly affect the optimization process.\n",
        "Machine learning practitioners often perform hyperparameter tuning to find the optimal learning rate and other hyperparameters that lead to faster\n",
        "convergence and better model performance.\n",
        "Stochastic Gradient Descent (SGDStochastic Gradient Descent is a variation of gradient descent that uses a random subset of training data (a mini-batch) to\n",
        "compute the gradient at each iteration. This can significantly speed up training, especially for large datasets.\n",
        "\n",
        "Regularization: Gradient descent can be extended to include regularization terms (such as L1 or L2 regularization) in the loss function to prevent\n",
        "overfitting and encourage simpler models."
      ],
      "metadata": {
        "id": "iM-rJGKIBuXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?\n",
        "Multiple linear regression is a statistical modeling technique used to analyze the relationship between a dependent variable (Y) and two or more\n",
        "independent variables (X1, X2, X3, ..., Xn). It is an extension of simple linear regression, which deals with only one independent variable.\n",
        "Multiple linear regression allows for a more complex modeling of the relationships between variables, considering the combined effects of multiple predictors."
      ],
      "metadata": {
        "id": "wb7_GdybDisd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and\n",
        "address this issue?\n",
        "\n",
        "Multicollinearity is a common issue that can arise in multiple linear regression when two or more independent variables in the model are highly correlated\n",
        "with each other. This high correlation between predictor variables can cause problems in the regression analysis, making it difficult to\n",
        "determine the individual effect of each predictor on the dependent variable.\n",
        "Detection of Multicollinearity:\n",
        "There are several ways to detect multicollinearity in a multiple linear regression model:\n",
        "\n",
        "Correlation Matrix: Calculate the correlation coefficients between all pairs of independent variables. High absolute correlation values (close to 1 or -1)\n",
        "indicate potential multicollinearity."
      ],
      "metadata": {
        "id": "QVJNlqixD21a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q7. Describe the polynomial regression model. How is it different from linear regression?\n",
        "\n",
        "Key Differences Between Polynomial Regression and Linear Regression:\n",
        "Linearity:\n",
        "Linear Regression: Assumes a linear relationship between the dependent and independent variables. The relationship is represented as a straight line.\n",
        "Polynomial Regression: Allows for nonlinear relationships by including higher-order polynomial terms (e.g., quadratic, cubic) to capture curvature in the data.\n",
        "\n",
        "Model Complexity:\n",
        "Linear Regression: Simpler model with a straight-line relationship.\n",
        "Polynomial Regression: More complex model with the ability to capture complex, nonlinear patterns in the data.\n",
        "\n",
        "Interpretation:\n",
        "In linear regression, the coefficients (β1) represent the change in the dependent variable for a one-unit change in the independent variable.\n",
        "In polynomial regression, the interpretation of coefficients becomes more complex, as each coefficient is associated with a particular polynomial term.\n",
        "For example, β1 represents the change in the dependent variable for a one-unit change in X in the linear term, while β2 represents the change associated\n",
        "with the quadratic term (X^2), and so on.\n",
        "\n",
        "Use Cases:\n",
        "\n",
        "Linear Regression: Suitable for modeling relationships that are primarily linear.\n",
        "Polynomial Regression: Appropriate when the relationship between variables is nonlinear or when you need to capture and model curvature in the data.\n"
      ],
      "metadata": {
        "id": "ntGNABemEhjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q8. What are the advantages and disadvantages of polynomial regression compared to linear\n",
        "regression? In what situations would you prefer to use polynomial regression?\n",
        "\n",
        "Advantages of Polynomial Regression:\n",
        "\n",
        "Captures Nonlinearity: Polynomial regression can model nonlinear relationships between the dependent and independent variables.\n",
        "It can capture curves, bends, and complex patterns that linear regression cannot.\n",
        "\n",
        "Flexibility: By including higher-order polynomial terms, you can better fit the data to its underlying behavior.\n",
        "This flexibility allows you to model a wide range of relationships.\n",
        "\n",
        "Improved Accuracy: In situations where the relationship is truly nonlinear, polynomial regression can provide more accurate predictions\n",
        "compared to linear regression.\n",
        "\n",
        "Disadvantages of Polynomial Regression:\n",
        "\n",
        "Overfitting: Using high-degree polynomial terms can lead to overfitting, where the model fits the training data too closely and performs poorly on new,\n",
        "unseen data. Care must be taken to choose an appropriate degree for the polynomial.\n",
        "\n",
        "Complexity: As the degree of the polynomial increases, the model becomes more complex and harder to interpret. It may become challenging to explain the\n",
        "meaning of the coefficients.\n",
        "\n",
        "Lack of Extrapolation: Polynomial regression is not suitable for extrapolation beyond the range of the observed data.\n",
        "Predictions outside this range may not be reliable.\n",
        "\n",
        "Increased Variance: High-degree polynomials can introduce high variance in the model, making it sensitive to small changes in the data."
      ],
      "metadata": {
        "id": "y4DsRHCvFJaM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}