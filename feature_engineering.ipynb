{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxeI76C+htR5sHKJm3NXkR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juhi3101/ml_libraries/blob/main/feature_engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SutSSuAIWBmr"
      },
      "outputs": [],
      "source": [
        "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
        "application.\n",
        "\n",
        "Min-Max scaling, also known as normalization, is a data preprocessing technique used in machine learning and statistics to scale and transform numerical\n",
        "features in a dataset to a specific range, typically between 0 and 1. This scaling method is useful when you have features with different units or scales,\n",
        "and you want to ensure that they all contribute equally to the model without one dominating due to its larger magnitude.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
        "Provide an example to illustrate its application.\n",
        "\n",
        "The Unit Vector technique in feature scaling, also known as \"Unit Normalization\" or \"Vector Normalization,\" is a method used to scale numerical\n",
        "features in a dataset such that each feature has a Euclidean norm (magnitude) of 1. This technique is particularly useful when you want to maintain\n",
        "the direction of the original data vectors while ensuring that they all have the same length.\n",
        "\n"
      ],
      "metadata": {
        "id": "V3vYKquZW_b9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
        "example to illustrate its application.\n",
        "\n",
        "\n",
        "Principal Component Analysis (PCA) is a dimensionality reduction technique used in statistics and machine learning to transform high-dimensional data\n",
        "into a lower-dimensional representation while preserving as much of the original data's variance as possible. It achieves this by identifying the principal\n",
        "components, which are linear combinations of the original features that capture the most significant variance in the data. PCA is widely used for data\n",
        "compression, visualization, and noise reduction.\n",
        "\n",
        "Example: Dimensionality Reduction in Face Recognition\n",
        "\n",
        "Suppose you have a dataset of grayscale images of faces, each represented as a grid of pixel values. Each image has a high dimensionality because\n",
        "each pixel is treated as a separate feature. However, you want to reduce this dimensionality while preserving the most important facial features for\n",
        "face recognition.\n",
        "\n",
        "Data Preparation: Standardize the pixel values of the images to have a mean of 0 and a standard deviation of 1.\n",
        "\n",
        "Covariance Matrix: Compute the covariance matrix of the standardized image dataset. This matrix captures how the pixel values of the images covary with\n",
        "each other.\n",
        "\n",
        "Eigenvalue Decomposition: Perform eigenvalue decomposition on the covariance matrix to obtain the eigenvalues and eigenvectors.\n",
        "\n",
        "Select Principal Components: Sort the eigenvectors in descending order of their corresponding eigenvalues. The eigenvectors with the largest eigenvalues\n",
        "represent the most important directions of variation in the data. You can choose to keep only the top N eigenvectors to reduce the dimensionality to N.\n",
        "\n",
        "Dimensionality Reduction: Project the standardized images onto the selected principal components. Each image is now represented as a\n",
        "linear combination of these principal components, effectively reducing the dimensionality.\n",
        "The reduced-dimensional representation can be used for tasks like face recognition, where you can compare the lower-dimensional representations of different\n",
        "faces, making the recognition process computationally more efficient while maintaining the essential facial characteristics.\n",
        "\n"
      ],
      "metadata": {
        "id": "AaaX6__sD2ES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
        "Extraction? Provide an example to illustrate this concept.\n",
        "\n",
        "PCA (Principal Component Analysis) can be used as a feature extraction technique in machine learning and data analysis.\n",
        "Feature extraction is the process of selecting or transforming the most informative features from a high-dimensional dataset. PCA helps achieve\n",
        "this by identifying and creating a new set of features (principal components) that capture the most significant variations in the data. The relationship\n",
        "between PCA and feature extraction lies in how PCA extracts these informative features from the original data.\n",
        "\n",
        "Here's how PCA can be used for feature extraction and an example to illustrate this concept:\n",
        "\n",
        "Feature Extraction with PCA:\n",
        "\n",
        "Data Preparation: Standardize the dataset if necessary to ensure that all features have a mean of 0 and a standard deviation of 1.\n",
        "\n",
        "PCA Transformation: Apply PCA to the standardized dataset. PCA will compute the covariance matrix and identify the principal components.\n",
        "\n",
        "Select Principal Components: Choose a subset of the principal components that capture a desired amount of variance in the data.\n",
        "You can select a fixed number of components or specify a threshold for the variance explained (e.g., keeping components until they explain 95% of the variance).\n",
        "\n",
        "Transform Data: Project the original data onto the selected principal components. This transformation results in a lower-dimensional representation of the data,\n",
        "where each instance is represented by its values along the chosen principal components. These transformed components serve as the new features.\n",
        "\n",
        "Reduced-Dimensional Data: The reduced-dimensional data contains a smaller number of features (the selected principal components) compared to the original\n",
        "dataset. These features are typically uncorrelated and capture the most significant patterns or variations in the data.\n"
      ],
      "metadata": {
        "id": "0vJxN0Q8E0VY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9UcCxmBLFbWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0U1Yih8wFbUn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}